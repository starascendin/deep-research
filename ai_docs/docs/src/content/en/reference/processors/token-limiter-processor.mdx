---
title: "Reference: Token Limiter Processor | Processors | Mastra Docs"
description: "Documentation for the TokenLimiterProcessor in Mastra, which limits the number of tokens in AI responses."
---

# TokenLimiterProcessor

The `TokenLimiterProcessor` is an **output processor** that limits the number of tokens in AI responses. This processor helps control response length by implementing token counting with configurable strategies for handling exceeded limits, including truncation and abortion options for both streaming and non-streaming scenarios.

## Usage example

```typescript copy
import { TokenLimiterProcessor } from "@mastra/core/processors";

const processor = new TokenLimiterProcessor({
  limit: 1000,
  strategy: "truncate",
  countMode: "cumulative"
});
```

## Constructor parameters

<PropertiesTable
  content={[
    {
      name: "options",
      type: "number | Options",
      description: "Either a simple number for token limit, or configuration options object",
      isOptional: false,
    },
  ]}
/>

### Options

<PropertiesTable
  content={[
    {
      name: "limit",
      type: "number",
      description: "Maximum number of tokens to allow in the response",
      isOptional: false,
    },
    {
      name: "encoding",
      type: "TiktokenBPE",
      description: "Optional encoding to use. Defaults to o200k_base which is used by gpt-4o",
      isOptional: true,
      default: "o200k_base",
    },
    {
      name: "strategy",
      type: "'truncate' | 'abort'",
      description: "Strategy when token limit is reached: 'truncate' stops emitting chunks, 'abort' calls abort() to stop the stream",
      isOptional: true,
      default: "'truncate'",
    },
    {
      name: "countMode",
      type: "'cumulative' | 'part'",
      description: "Whether to count tokens from the beginning of the stream or just the current part: 'cumulative' counts all tokens from start, 'part' only counts tokens in current part",
      isOptional: true,
      default: "'cumulative'",
    },
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "name",
      type: "string",
      description: "Processor name set to 'token-limiter'",
      isOptional: false,
    },
    {
      name: "processOutputStream",
      type: "(args: { part: ChunkType; streamParts: ChunkType[]; state: Record<string, any>; abort: (reason?: string) => never }) => Promise<ChunkType | null>",
      description: "Processes streaming output parts to limit token count during streaming",
      isOptional: false,
    },
    {
      name: "processOutputResult",
      type: "(args: { messages: MastraMessageV2[]; abort: (reason?: string) => never }) => Promise<MastraMessageV2[]>",
      description: "Processes final output results to limit token count in non-streaming scenarios",
      isOptional: false,
    },
    {
      name: "reset",
      type: "() => void",
      description: "Reset the token counter (useful for testing or reusing the processor)",
      isOptional: false,
    },
    {
      name: "getCurrentTokens",
      type: "() => number",
      description: "Get the current token count",
      isOptional: false,
    },
    {
      name: "getMaxTokens",
      type: "() => number",
      description: "Get the maximum token limit",
      isOptional: false,
    },
  ]}
/>

## Extended usage example

```typescript filename="src/mastra/agents/limited-agent.ts" showLineNumbers copy
import { openai } from "@ai-sdk/openai";
import { Agent } from "@mastra/core/agent";
import { TokenLimiterProcessor } from "@mastra/core/processors";

export const agent = new Agent({
  name: "limited-agent",
  instructions: "You are a helpful assistant",
  model: openai("gpt-4o-mini"),
  outputProcessors: [
    new TokenLimiterProcessor({
      limit: 1000,
      strategy: "truncate",
      countMode: "cumulative"
    })
  ]
});
```

## Related

- [Input Processors](/docs/agents/input-processors)
- [Output Processors](/docs/agents/output-processors)
