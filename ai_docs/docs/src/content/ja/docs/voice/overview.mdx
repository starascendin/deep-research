---
title: Mastra の音声機能 | Mastra Docs
description: Mastra の音声機能の概要。テキスト読み上げ、音声認識、リアルタイムの音声対話を含みます。
---

import { Tabs } from "nextra/components";
import { AudioPlayback } from "@/components/audio-playback";



# Mastra における Voice

Mastra の Voice システムは、音声インタラクション向けの統一されたインターフェースを提供し、アプリケーションでの text-to-speech (TTS)、speech-to-text (STT)、およびリアルタイムの speech-to-speech (STS) を実現します。



## エージェントに音声を追加する

エージェントに音声機能を組み込む方法は、[Adding Voice to Agents](../agents/adding-voice.mdx) のドキュメントをご参照ください。このセクションでは、単一・複数の音声プロバイダの使い分けやリアルタイム対話の実装方法を解説します。

```typescript
import { Agent } from "@mastra/core/agent";
import { openai } from "@ai-sdk/openai";
import { OpenAIVoice } from "@mastra/voice-openai";

// TTS 用に OpenAI の音声を初期化

const voiceAgent = new Agent({
  name: "Voice Agent",
  instructions:
    "You are a voice assistant that can help users with their tasks.",
  model: openai("gpt-4o"),
  voice: new OpenAIVoice(),
});
```

続いて、以下の音声機能が利用できます。

### Text to Speech (TTS)

Mastra の TTS 機能を使って、エージェントの応答を自然な音声に変換します。
OpenAI、ElevenLabs など、複数のプロバイダから選べます。

詳細な設定や高度な機能については、[Text-to-Speech ガイド](./text-to-speech) をご覧ください。

{/*
LLM CONTEXT: この Tabs コンポーネントは、複数の音声プロバイダにおける Text-to-Speech (TTS) の実装例を示します。
各タブでは、Mastra エージェントで特定の TTS プロバイダ（OpenAI、Azure、ElevenLabs など）を設定して利用する方法を紹介します。
タブは、ユーザーが各 TTS プロバイダを比較し、ニーズに最適なものを選ぶ際の助けになります。
各タブには、エージェントのセットアップ、テキスト生成、音声再生までを含む完全なコード例が掲載されています。
プロバイダには、OpenAI、Azure、ElevenLabs、PlayAI、Google、Cloudflare、Deepgram、Speechify、Sarvam、Murf が含まれます。
*/}



<Tabs items={["OpenAI", "Azure", "ElevenLabs", "PlayAI", "Google", "Cloudflare", "Deepgram", "Speechify", "Sarvam", "Murf"]}>
  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { OpenAIVoice } from "@mastra/voice-openai";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new OpenAIVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
    speaker: "default", // Optional: specify a speaker
    responseFormat: "wav", // Optional: specify a response format
    });

    playAudio(audioStream);

    ```

    OpenAI の音声プロバイダーについては、[OpenAI Voice Reference](/reference/voice/openai) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { AzureVoice } from "@mastra/voice-azure";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new AzureVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "en-US-JennyNeural", // Optional: specify a speaker
    });

    playAudio(audioStream);
    ```

    Azure の音声プロバイダーについては、[Azure Voice Reference](/reference/voice/azure) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { ElevenLabsVoice } from "@mastra/voice-elevenlabs";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new ElevenLabsVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
    speaker: "default", // Optional: specify a speaker
    });

    playAudio(audioStream);

    ```

    ElevenLabs の音声プロバイダーについては、[ElevenLabs Voice Reference](/reference/voice/elevenlabs) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { PlayAIVoice } from "@mastra/voice-playai";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new PlayAIVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // Optional: specify a speaker
    });

    playAudio(audioStream);
    ```

    PlayAI の音声プロバイダーについては、[PlayAI Voice Reference](/reference/voice/playai) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { GoogleVoice } from "@mastra/voice-google";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new GoogleVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
    speaker: "en-US-Studio-O", // Optional: specify a speaker
    });

    playAudio(audioStream);

    ```

    Google の音声プロバイダーについて詳しくは、[Google Voice Reference](/reference/voice/google) をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { CloudflareVoice } from "@mastra/voice-cloudflare";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new CloudflareVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // Optional: specify a speaker
    });

    playAudio(audioStream);
    ```

    Cloudflare の音声プロバイダーについて詳しくは、[Cloudflare Voice Reference](/reference/voice/cloudflare) をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { DeepgramVoice } from "@mastra/voice-deepgram";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new DeepgramVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
    speaker: "aura-english-us", // Optional: specify a speaker
    });

    playAudio(audioStream);

    ```

    Deepgram の音声プロバイダーについて詳しくは、[Deepgram Voice Reference](/reference/voice/deepgram) をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { SpeechifyVoice } from "@mastra/voice-speechify";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new SpeechifyVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "matthew", // Optional: specify a speaker
    });

    playAudio(audioStream);
    ```

    Speechify の音声プロバイダーについて詳しくは、[Speechify Voice Reference](/reference/voice/speechify) をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { SarvamVoice } from "@mastra/voice-sarvam";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new SarvamVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // Convert text to speech to an Audio Stream
    const audioStream = await voiceAgent.voice.speak(text, {
    speaker: "default", // Optional: specify a speaker
    });

    playAudio(audioStream);

    ```

    Sarvam の音声プロバイダーについて詳しくは、[Sarvam Voice Reference](/reference/voice/sarvam) をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { MurfVoice } from "@mastra/voice-murf";
    import { playAudio } from "@mastra/node-audio";

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new MurfVoice(),
    });

    const { text } = await voiceAgent.generate('What color is the sky?');

    // テキストを音声に変換し、オーディオストリームを生成
    const audioStream = await voiceAgent.voice.speak(text, {
      speaker: "default", // 任意: 話者を指定
    });

    playAudio(audioStream);
    ```

    Murf の音声プロバイダーの詳細については、[Murf Voice Reference](/reference/voice/murf) を参照してください。
  </Tabs.Tab>
</Tabs>



### 音声認識（STT）

OpenAI、ElevenLabs などの各種プロバイダーを利用して、音声コンテンツをテキストに変換します。詳細な設定オプションについては、[Speech to Text](./speech-to-text) をご覧ください。

サンプル音声ファイルは[こちら](https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3)からダウンロードできます。

<br />
<AudioPlayback audio="https://github.com/mastra-ai/realtime-voice-demo/raw/refs/heads/main/how_can_i_help_you.mp3" />

{/*
LLM CONTEXT: この Tabs コンポーネントは、複数の音声プロバイダーにおける Speech-to-Text（STT）の実装例を示します。
各タブでは、特定の STT プロバイダーのセットアップ方法と、音声をテキストへ変換する手順を説明します。
このタブにより、異なるプロバイダーで音声認識を実装する方法を理解できます。
各タブには、音声ファイルの取り扱い、文字起こし、レスポンス生成を示すコード例が含まれています。
*/}



<Tabs items={["OpenAI", "Azure", "ElevenLabs", "Google", "Cloudflare", "Deepgram", "Sarvam"]}>
  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { OpenAIVoice } from "@mastra/voice-openai";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new OpenAIVoice(),
    });

    // URL からの音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // 文字起こしに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);

    ```

    OpenAI の音声プロバイダーの詳細は、[OpenAI Voice Reference](/reference/voice/openai) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { createReadStream } from 'fs';
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { AzureVoice } from "@mastra/voice-azure";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new AzureVoice(),
    });

    // URL からの音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // 文字起こしに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Azure の音声プロバイダーの詳細は、[Azure Voice Reference](/reference/voice/azure) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { ElevenLabsVoice } from "@mastra/voice-elevenlabs";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new ElevenLabsVoice(),
    });

    // URL からの音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // 文字起こしに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);

    ```

    ElevenLabs の音声プロバイダーの詳細は、[ElevenLabs Voice Reference](/reference/voice/elevenlabs) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { GoogleVoice } from "@mastra/voice-google";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new GoogleVoice(),
    });

    // URL からの音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // 文字起こしに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Google の音声プロバイダーの詳細は、[Google Voice Reference](/reference/voice/google) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { CloudflareVoice } from "@mastra/voice-cloudflare";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new CloudflareVoice(),
    });

    // URL から取得した音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // テキストに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);

    ```

    Cloudflare の音声プロバイダの詳細は、[Cloudflare Voice Reference](/reference/voice/cloudflare)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { DeepgramVoice } from "@mastra/voice-deepgram";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
      name: "Voice Agent",
      instructions: "You are a voice assistant that can help users with their tasks.",
      model: openai("gpt-4o"),
      voice: new DeepgramVoice(),
    });

    // URL から取得した音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // テキストに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);
    ```

    Deepgram の音声プロバイダの詳細は、[Deepgram Voice Reference](/reference/voice/deepgram)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    import { Agent } from '@mastra/core/agent';
    import { openai } from '@ai-sdk/openai';
    import { SarvamVoice } from "@mastra/voice-sarvam";
    import { createReadStream } from 'fs';

    const voiceAgent = new Agent({
    name: "Voice Agent",
    instructions: "You are a voice assistant that can help users with their tasks.",
    model: openai("gpt-4o"),
    voice: new SarvamVoice(),
    });

    // URL から取得した音声ファイルを使用
    const audioStream = await createReadStream("./how_can_i_help_you.mp3");

    // 音声をテキストに変換
    const transcript = await voiceAgent.voice.listen(audioStream);
    console.log(`User said: ${transcript}`);

    // テキストに基づいて応答を生成
    const { text } = await voiceAgent.generate(transcript);

    ```

    Sarvam の音声プロバイダの詳細は、[Sarvam Voice Reference](/reference/voice/sarvam)をご覧ください。
  </Tabs.Tab>
</Tabs>



### 音声対話（STS）

音声同士のやり取りで自然な会話体験を実現します。統合APIにより、ユーザーとAIエージェントの間でリアルタイムの音声インタラクションが可能です。
詳細な設定オプションや高度な機能については、[Speech to Speech](./speech-to-speech)をご覧ください。

{/*
  LLM CONTEXT: この Tabs コンポーネントは、リアルタイムの音声インタラクション向けの Speech-to-Speech (STS) 実装を示します。
  現在は、双方向の音声会話のための OpenAI のリアルタイム音声実装のみを表示しています。
  タブでは、音声応答のイベント処理を含むリアルタイム音声通信のセットアップ方法を示します。
  これにより、連続的な音声ストリーミングを伴う会話型AI体験が可能になります。
*/}
<Tabs items={["OpenAI", "Google Gemini Live"]}>
  <Tabs.Tab>
```typescript
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { playAudio, getMicrophoneStream } from '@mastra/node-audio';
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";

const voiceAgent = new Agent({
  name: "Voice Agent",
  instructions: "You are a voice assistant that can help users with their tasks.",
  model: openai("gpt-4o"),
  voice: new OpenAIRealtimeVoice(),
});

// Listen for agent audio responses
voiceAgent.voice.on('speaker', ({ audio }) => {
  playAudio(audio);
});

// Initiate the conversation
await voiceAgent.voice.speak('How can I help you today?');

// Send continuous audio from the microphone
const micStream = getMicrophoneStream();
await voiceAgent.voice.send(micStream);
````

OpenAI の音声プロバイダの詳細は、[OpenAI Voice Reference](/reference/voice/openai-realtime)をご覧ください。

  </Tabs.Tab>
  <Tabs.Tab>
```typescript
import { Agent } from '@mastra/core/agent';
import { openai } from '@ai-sdk/openai';
import { playAudio, getMicrophoneStream } from '@mastra/node-audio';
import { GeminiLiveVoice } from "@mastra/voice-google-gemini-live";

const voiceAgent = new Agent({
  name: "Voice Agent",
  instructions: "You are a voice assistant that can help users with their tasks.",
  model: openai("gpt-4o"),
  voice: new GeminiLiveVoice({
    // Live API mode
    apiKey: process.env.GOOGLE_API_KEY,
    model: 'gemini-2.0-flash-exp',
    speaker: 'Puck',
    debug: true,
    // Vertex AI alternative:
    // vertexAI: true,
    // project: 'your-gcp-project',
    // location: 'us-central1',
    // serviceAccountKeyFile: '/path/to/service-account.json',
  }),
});

// Connect before using speak/send
await voiceAgent.voice.connect();

// Listen for agent audio responses
voiceAgent.voice.on('speaker', ({ audio }) => {
  playAudio(audio);
});

// Listen for text responses and transcriptions
voiceAgent.voice.on('writing', ({ text, role }) => {
  console.log(`${role}: ${text}`);
});

// Initiate the conversation
await voiceAgent.voice.speak('How can I help you today?');

// Send continuous audio from the microphone
const micStream = getMicrophoneStream();
await voiceAgent.voice.send(micStream);
```

Google Gemini Live の音声プロバイダの詳細は、[Google Gemini Live Reference](/reference/voice/google-gemini-live)をご覧ください。

  </Tabs.Tab>
</Tabs>



## 音声設定

各音声プロバイダーは、異なるモデルやオプションで構成できます。以下に、サポートされているすべてのプロバイダーの詳細な設定オプションを示します。

{/*
LLM CONTEXT: この Tabs コンポーネントは、サポートされているすべての音声プロバイダーの詳細な設定オプションを表示します。
各タブは、利用可能なすべてのオプションと設定を用いて特定の音声プロバイダーを設定する方法を示します。
タブは、モデル、言語、詳細設定を含む各プロバイダーの設定機能を網羅的に理解するのに役立ちます。
該当する場合、各タブは音声合成モデルとリスニングモデルの両方の設定を示します。
*/}



<Tabs items={["OpenAI", "Azure", "ElevenLabs", "PlayAI", "Google", "Cloudflare", "Deepgram", "Speechify", "Sarvam", "Murf", "OpenAI Realtime", "Google Gemini Live"]}>
  <Tabs.Tab>
    ```typescript
    // OpenAI Voice の設定
    const voice = new OpenAIVoice({
      speechModel: {
        name: "gpt-3.5-turbo", // 例: モデル名
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
        voiceType: "neural", // 音声モデルのタイプ
      },
      listeningModel: {
        name: "whisper-1", // 例: モデル名
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
        format: "wav", // 音声フォーマット
      },
      speaker: "alloy", // 例: 話者名
    });
    ```

    OpenAI の音声プロバイダーの詳細は、[OpenAI Voice リファレンス](/reference/voice/openai)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Azure Voice の設定
    const voice = new AzureVoice({
      speechModel: {
        name: "en-US-JennyNeural", // 例: モデル名
        apiKey: process.env.AZURE_SPEECH_KEY,
        region: process.env.AZURE_SPEECH_REGION,
        language: "en-US", // 言語コード
        style: "cheerful", // ボイスタイル
        pitch: "+0Hz", // ピッチ調整
        rate: "1.0", // 話速
      },
      listeningModel: {
        name: "en-US", // 例: モデル名
        apiKey: process.env.AZURE_SPEECH_KEY,
        region: process.env.AZURE_SPEECH_REGION,
        format: "simple", // 出力形式
      },
    });
    ```

    Azure の音声プロバイダーの詳細は、[Azure Voice リファレンス](/reference/voice/azure)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // ElevenLabs Voice の設定
    const voice = new ElevenLabsVoice({
      speechModel: {
        voiceId: "your-voice-id", // 例: ボイスID
        model: "eleven_multilingual_v2", // 例: モデル名
        apiKey: process.env.ELEVENLABS_API_KEY,
        language: "en", // 言語コード
        emotion: "neutral", // 感情設定
      },
      // ElevenLabs にはリスニングモデルが別途ない場合があります
    });
    ```

    ElevenLabs の音声プロバイダーの詳細は、[ElevenLabs Voice リファレンス](/reference/voice/elevenlabs)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // PlayAI Voice の設定
    const voice = new PlayAIVoice({
      speechModel: {
        name: "playai-voice", // 例: モデル名
        speaker: "emma", // 例: 話者名
        apiKey: process.env.PLAYAI_API_KEY,
        language: "en-US", // 言語コード
        speed: 1.0, // 発話速度
      },
      // PlayAI にはリスニングモデルが別途ない場合があります
    });
    ```

    PlayAI の音声プロバイダーの詳細は、[PlayAI Voice リファレンス](/reference/voice/playai)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Google Voice の設定
    const voice = new GoogleVoice({
      speechModel: {
        name: "en-US-Studio-O", // 例: モデル名
        apiKey: process.env.GOOGLE_API_KEY,
        languageCode: "en-US", // 言語コード
        gender: "FEMALE", // 声質（性別）
        speakingRate: 1.0, // 発話速度
      },
      listeningModel: {
        name: "en-US", // 例: モデル名
        sampleRateHertz: 16000, // サンプルレート
      },
    });
    ```

    Google の音声プロバイダーの詳細は、[Google Voice リファレンス](/reference/voice/google)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Cloudflare Voice の設定
    const voice = new CloudflareVoice({
      speechModel: {
        name: "cloudflare-voice", // 例: モデル名
        accountId: process.env.CLOUDFLARE_ACCOUNT_ID,
        apiToken: process.env.CLOUDFLARE_API_TOKEN,
        language: "en-US", // 言語コード
        format: "mp3", // 音声フォーマット
      },
      // Cloudflare にはリスニングモデルが別途ない場合があります
    });
    ```

    Cloudflare の音声プロバイダーの詳細は、[Cloudflare Voice リファレンス](/reference/voice/cloudflare)をご覧ください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Deepgram Voice の構成
    const voice = new DeepgramVoice({
      speechModel: {
        name: "nova-2", // モデル名の例
        speaker: "aura-english-us", // 話者名の例
        apiKey: process.env.DEEPGRAM_API_KEY,
        language: "en-US", // 言語コード
        tone: "formal", // 口調設定
      },
      listeningModel: {
        name: "nova-2", // モデル名の例
        format: "flac", // 音声フォーマット
      },
    });
    ```

    Deepgram の音声プロバイダーの詳細は、[Deepgram Voice Reference](/reference/voice/deepgram) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Speechify Voice の構成
    const voice = new SpeechifyVoice({
      speechModel: {
        name: "speechify-voice", // モデル名の例
        speaker: "matthew", // 話者名の例
        apiKey: process.env.SPEECHIFY_API_KEY,
        language: "en-US", // 言語コード
        speed: 1.0, // 話速
      },
      // Speechify には別途リスニングモデルがない場合があります
    });
    ```

    Speechify の音声プロバイダーの詳細は、[Speechify Voice Reference](/reference/voice/speechify) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Sarvam Voice の構成
    const voice = new SarvamVoice({
      speechModel: {
        name: "sarvam-voice", // モデル名の例
        apiKey: process.env.SARVAM_API_KEY,
        language: "en-IN", // 言語コード
        style: "conversational", // スタイル設定
      },
      // Sarvam には別途リスニングモデルがない場合があります
    });
    ```

    Sarvam の音声プロバイダーの詳細は、[Sarvam Voice Reference](/reference/voice/sarvam) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Murf Voice の構成
    const voice = new MurfVoice({
      speechModel: {
        name: "murf-voice", // モデル名の例
        apiKey: process.env.MURF_API_KEY,
        language: "en-US", // 言語コード
        emotion: "happy", // 感情設定
      },
      // Murf には別途リスニングモデルがない場合があります
    });
    ```

    Murf の音声プロバイダーの詳細は、[Murf Voice Reference](/reference/voice/murf) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // OpenAI Realtime Voice の構成
    const voice = new OpenAIRealtimeVoice({
      speechModel: {
        name: "gpt-3.5-turbo", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        language: "en-US", // 言語コード
      },
      listeningModel: {
        name: "whisper-1", // モデル名の例
        apiKey: process.env.OPENAI_API_KEY,
        format: "ogg", // 音声フォーマット
      },
      speaker: "alloy", // 話者名の例
    });
    ```

    OpenAI Realtime の音声プロバイダーの詳細は、[OpenAI Realtime Voice Reference](/reference/voice/openai-realtime) を参照してください。
  </Tabs.Tab>

  <Tabs.Tab>
    ```typescript
    // Google Gemini Live Voice の構成
    const voice = new GeminiLiveVoice({
      speechModel: {
        name: "gemini-2.0-flash-exp", // モデル名の例
        apiKey: process.env.GOOGLE_API_KEY,
      },
      speaker: "Puck", // 話者名の例
      // Google Gemini Live は、音声生成とリスニングのモデルが分かれていない双方向リアルタイム API です
    });
    ```

    Google Gemini Live の音声プロバイダーの詳細は、[Google Gemini Live Reference](/reference/voice/google-gemini-live) を参照してください。
  </Tabs.Tab>
</Tabs>



### 複数の音声プロバイダの利用

この例では、Mastra で OpenAI を音声認識（STT）に、PlayAI を音声合成（TTS）に用い、2 つの異なる音声プロバイダを作成して使う方法を示します。

まず、必要な設定を指定して音声プロバイダのインスタンスを作成します。

```typescript
import { OpenAIVoice } from "@mastra/voice-openai";
import { PlayAIVoice } from "@mastra/voice-playai";
import { CompositeVoice } from "@mastra/core/voice";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

// STT 用に OpenAI の音声を初期化
const input = new OpenAIVoice({
  listeningModel: {
    name: "whisper-1",
    apiKey: process.env.OPENAI_API_KEY,
  },
});

// TTS 用に PlayAI の音声を初期化
const output = new PlayAIVoice({
  speechModel: {
    name: "playai-voice",
    apiKey: process.env.PLAYAI_API_KEY,
  },
});

// CompositeVoice でプロバイダを統合
const voice = new CompositeVoice({
  input,
  output,
});

// 統合した音声プロバイダで対話を実装
const audioStream = getMicrophoneStream(); // この関数で音声入力を取得すると仮定
const transcript = await voice.listen(audioStream);

// 文字起こし結果をログ出力
console.log("Transcribed text:", transcript);

// テキストを音声に変換
const responseAudio = await voice.speak(`You said: ${transcript}`, {
  speaker: "default", // 任意: 話者を指定
  responseFormat: "wav", // 任意: 返却フォーマットを指定
});

// 音声応答を再生
playAudio(responseAudio);
```

CompositeVoice の詳細は、[CompositeVoice Reference](/reference/voice/composite-voice)を参照してください。



## 参考資料

- [CompositeVoice](../../reference/voice/composite-voice.mdx)
- [MastraVoice](../../reference/voice/mastra-voice.mdx)
- [OpenAI Voice](../../reference/voice/openai.mdx)
- [OpenAI Realtime Voice](../../reference/voice/openai-realtime.mdx)
- [Azure Voice](../../reference/voice/azure.mdx)
- [Google Voice](../../reference/voice/google.mdx)
- [Google Gemini Live Voice](../../reference/voice/google-gemini-live.mdx)
- [Deepgram Voice](../../reference/voice/deepgram.mdx)
- [PlayAI Voice](../../reference/voice/playai.mdx)
- [音声のサンプル](../../examples/voice/text-to-speech.mdx)
