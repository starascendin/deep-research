---
title: Mastraにおける音声対音声機能 | Mastra ドキュメント
description: Mastraの音声対音声機能の概要。リアルタイムの対話やイベント駆動型アーキテクチャについて説明します。
---



# Mastraの音声対音声機能



## はじめに

MastraのSpeech-to-Speech（STS）は、複数のプロバイダー間でリアルタイムなやり取りを可能にする標準化されたインターフェースを提供します。  
STSは、Realtimeモデルからのイベントをリッスンすることで、継続的な双方向音声通信を実現します。個別のTTSやSTT操作とは異なり、STSは両方向の音声を継続的に処理するオープンな接続を維持します。



## 設定

- **`apiKey`**: あなたのOpenAI APIキー。`OPENAI_API_KEY`環境変数にフォールバックします。
- **`model`**: リアルタイム音声対話に使用するモデルID（例：`gpt-4o-mini-realtime`）。
- **`speaker`**: 音声合成のデフォルトボイスID。音声出力に使用する声を指定できます。

```typescript
const voice = new OpenAIRealtimeVoice({
  apiKey: "your-openai-api-key",
  model: "gpt-4o-mini-realtime",
  speaker: "alloy", // Default voice
});

// If using default settings the configuration can be simplified to:
const voice = new OpenAIRealtimeVoice();
```



## STS の利用

```typescript
import { Agent } from "@mastra/core/agent";
import { OpenAIRealtimeVoice } from "@mastra/voice-openai-realtime";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

const agent = new Agent({
  name: "Agent",
  instructions: `You are a helpful assistant with real-time voice capabilities.`,
  model: openai("gpt-4o"),
  voice: new OpenAIRealtimeVoice(),
});

// 音声サービスに接続
await agent.voice.connect();

// エージェントの音声応答をリッスン
agent.voice.on("speaker", ({ audio }) => {
  playAudio(audio);
});

// 会話を開始
await agent.voice.speak("本日はいかがお手伝いできますか？");

// マイクからの連続音声を送信
const micStream = getMicrophoneStream();
await agent.voice.send(micStream);
```

エージェントに Speech-to-Speech 機能を統合するには、[Adding Voice to Agents](../agents/adding-voice.mdx) を参照してください。



## Google Gemini Live（リアルタイム）

```typescript
import { Agent } from "@mastra/core/agent";
import { GeminiLiveVoice } from "@mastra/voice-google-gemini-live";
import { playAudio, getMicrophoneStream } from "@mastra/node-audio";

const agent = new Agent({
  name: 'Agent',
  instructions: 'You are a helpful assistant with real-time voice capabilities.',
  // テキスト生成にはこのモデルを使用。リアルタイム音声は音声プロバイダ側で処理
  model: openai("gpt-4o"),
  voice: new GeminiLiveVoice({
    apiKey: process.env.GOOGLE_API_KEY,
    model: 'gemini-2.0-flash-exp',
    speaker: 'Puck',
    debug: true,
    // Vertex AI の場合のオプション:
    // vertexAI: true, 
    // project: 'your-gcp-project', 
    // location: 'us-central1',
    // serviceAccountKeyFile: '/path/to/service-account.json',
  }),
});

await agent.voice.connect();

agent.voice.on('speaker', ({ audio }) => {
  playAudio(audio);
});

agent.voice.on('writing', ({ role, text }) => {
  console.log(`${role}: ${text}`);
});

await agent.voice.speak('How can I help you today?');

const micStream = getMicrophoneStream();
await agent.voice.send(micStream);
```

Note:
- Live API には `GOOGLE_API_KEY` が必要です。Vertex AI を利用する場合は、project/location とサービスアカウントの認証情報が必要です。
- イベント: `speaker`（音声ストリーム）、`writing`（テキスト）、`turnComplete`、`usage`、`error`。
