---
title: "Reference: Answer Similarity | Scorers | Mastra Docs"
description: Documentation for the Answer Similarity Scorer in Mastra, which compares agent outputs against ground truth answers for CI/CD testing.
---

# Answer Similarity Scorer

The `createAnswerSimilarityScorer()` function creates a scorer that evaluates how similar an agent's output is to a ground truth answer. This scorer is specifically designed for CI/CD testing scenarios where you have expected answers and want to ensure consistency over time.

For usage examples, see the [Answer Similarity Examples](/examples/scorers/answer-similarity).

## Parameters

<PropertiesTable
  content={[
    {
      name: "model",
      type: "LanguageModel",
      required: true,
      description: "The language model used to evaluate semantic similarity between outputs and ground truth.",
    },
    {
      name: "options",
      type: "AnswerSimilarityOptions",
      required: false,
      description: "Configuration options for the scorer.",
    },
  ]}
/>

### AnswerSimilarityOptions

<PropertiesTable
  content={[
    {
      name: "requireGroundTruth",
      type: "boolean",
      required: false,
      defaultValue: "true",
      description: "Whether to require ground truth for evaluation. If false, missing ground truth returns score 0.",
    },
    {
      name: "semanticThreshold",
      type: "number",
      required: false,
      defaultValue: "0.8",
      description: "Weight for semantic matches vs exact matches (0-1).",
    },
    {
      name: "exactMatchBonus",
      type: "number",
      required: false,
      defaultValue: "0.2",
      description: "Additional score bonus for exact matches (0-1).",
    },
    {
      name: "missingPenalty",
      type: "number",
      required: false,
      defaultValue: "0.15",
      description: "Penalty per missing key concept from ground truth.",
    },
    {
      name: "contradictionPenalty",
      type: "number",
      required: false,
      defaultValue: "1.0",
      description: "Penalty for contradictory information. High value ensures wrong answers score near 0.",
    },
    {
      name: "extraInfoPenalty",
      type: "number",
      required: false,
      defaultValue: "0.05",
      description: "Mild penalty for extra information not present in ground truth (capped at 0.2).",
    },
    {
      name: "scale",
      type: "number",
      required: false,
      defaultValue: "1",
      description: "Score scaling factor.",
    },
  ]}
/>

This function returns an instance of the MastraScorer class. The `.run()` method accepts the same input as other scorers (see the [MastraScorer reference](./mastra-scorer)), but **requires ground truth** to be provided in the run object.

## .run() Returns

<PropertiesTable
  content={[
    {
      name: "runId",
      type: "string",
      description: "The id of the run (optional).",
    },
    {
      name: "score",
      type: "number",
      description: "Similarity score between 0-1 (or 0-scale if custom scale used). Higher scores indicate better similarity to ground truth.",
    },
    {
      name: "reason",
      type: "string",
      description: "Human-readable explanation of the score with actionable feedback.",
    },
    {
      name: "preprocessStepResult",
      type: "object",
      description: "Extracted semantic units from output and ground truth.",
    },
    {
      name: "analyzeStepResult",
      type: "object",
      description: "Detailed analysis of matches, contradictions, and extra information.",
    },
    {
      name: "preprocessPrompt",
      type: "string",
      description: "The prompt used for semantic unit extraction.",
    },
    {
      name: "analyzePrompt",
      type: "string",
      description: "The prompt used for similarity analysis.",
    },
    {
      name: "generateReasonPrompt",
      type: "string",
      description: "The prompt used for generating the explanation.",
    },
  ]}
/>

## Usage with runExperiment

This scorer is designed for use with `runExperiment` for CI/CD testing:

```typescript
import { runExperiment } from '@mastra/core/scores';
import { createAnswerSimilarityScorer } from '@mastra/evals/scorers/llm';

const scorer = createAnswerSimilarityScorer({ model });

await runExperiment({
  data: [
    { 
      input: "What is the capital of France?",
      groundTruth: "Paris is the capital of France"
    }
  ],
  scorers: [scorer],
  target: myAgent,
  onItemComplete: ({ scorerResults }) => {
    // Assert similarity score meets threshold
    expect(scorerResults['Answer Similarity Scorer'].score).toBeGreaterThan(0.8);
  }
});
```

## Key Features

- **Semantic Analysis**: Uses LLM to extract and compare semantic units rather than simple string matching
- **Contradiction Detection**: Identifies factually incorrect information and scores it near 0
- **Flexible Matching**: Supports exact, semantic, partial, and missing match types
- **CI/CD Ready**: Designed for automated testing with ground truth comparison
- **Actionable Feedback**: Provides specific explanations of what matched and what needs improvement

## Scoring Algorithm

The scorer uses a multi-step process:

1. **Extract**: Breaks down output and ground truth into semantic units
2. **Analyze**: Compares units and identifies matches, contradictions, and gaps
3. **Score**: Calculates weighted similarity with penalties for contradictions
4. **Reason**: Generates human-readable explanation

Score calculation: `max(0, base_score - contradiction_penalty - missing_penalty - extra_info_penalty) Ã— scale`